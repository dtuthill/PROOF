{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Retrieval by Omega Oscillation Filtering\n",
    "## Daniel Tuthill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the algorithm will not run correctly in this jupyter notebook. In order to run it, please use the proof_optimization.py script. Also note, this script requires command line inputs.\n",
    "\n",
    "Script below turns on equation numbering for markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Attosecond Phase Characterization\n",
    "\n",
    "When an ultrashort ($< 100 fs$), intense ($> 10^{14} \\frac{W}{cm^2}$) visible to infrared laser pulse is incident on an atom, a highly non-linear process known as High-Harmonic Generation (HHG) can occur which produces light at harmonics of the infrared driving frequency extending out to the 100s of eV level in photon energy (XUV to soft X-ray range). Due to this large increase in bandwidth, the resulting pulse produced is typically 100s of attoseconds long although with specialized techniques, pulses as short as 43 as have been produced [1]. When producing such short pulses, spectral intensities can be easily measured using a spectrometer; however, in order to fully characterize the pulse in order to retrieve the pulse duration or use it in other experimental techniques, the spectral phase must also be characterized. As the attosecond timescale is far beyond the limit of electronics, more involved interferometric techniques must be used.\n",
    "\n",
    "Most characterization techniques rely upon the fact that HHG produces harmonics of the driving frequency and not a continuous spectra, although again to be noted, with specific techniques, a continuous spectra can be produced. These harmonics though are typically only odd-harmonics of the infrared driving frequency due to the symmetry of the driving process (both the peak and trough of the driving electric field induce identical interactions). One of the most common characterizations of odd-harmonic pulses is the Reconstruction of Attosecond Beating by Two-photon Transitions (RABBITT) [2] which is used in the DiMauro group at Ohio State. The symmetry of HHG though can be broken when using an asymmetric driving field produced from the interference of an infrared fundamental frequency and its second harmonic as shown in the figure below which will then result in an attosecond pulse consisting of both even and odd harmonics. RABBITT unfortunately is unable to characterize even-old pulses and therefore other characterization techniques such as Phase Retrieval by Omega Oscillation Filtering (PROOF) [3] must be used.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Asymmetric Pulse.png\">\n",
    "\n",
    "***Figure:** Representative two-color field. The blue line is the fundamental field, $E_\\omega \\cos(\\omega t)$. The red line is the second harmonic field, $E_{2\\omega} \\cos(2\\omega t - \\phi)$. The black line is the two-color driving field. $E_{2\\omega}=0.35E_\\omega$ and $\\phi=0$.*\n",
    "\n",
    "[1]: Thomas Gaumnitz, Arohi Jain, Yoann Pertot, Martin Huppert, Inga Jordan, Fernando Ardana-Lamas, and Hans Jakob Wörner, \"Streaking of 43-attosecond soft-X-ray pulses generated by a passively CEP-stable mid-infrared driver,\" Opt. Express 25, 27506-27518 (2017)\n",
    "\n",
    "[2]: P. M. Paul, E. S. Toma, P. Breger, G. Mullot, F. Augé, P. Balcou, H. G. Muller, and P. Agostini, “Observation of\n",
    "a train of attosecond pulses from high harmonic generation,” Science 292(5522), 1689–1692 (2001). \n",
    "\n",
    "[3]: Chini, M., Gilbertson, S., Khan, S. D. & Chang, Z. Characterizing\n",
    "ultrabroadband attosecond lasers. Opt. Express 18, 13006–13016 (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Retrieval by Omega Oscillation Filtering (PROOF)\n",
    "## Introduction\n",
    "\n",
    "PROOF characterizes the phase of the attosecond pulse by inducing a quantum interference in a target gas using an additional infrared probe field. When the attosecond pulse, or here forward referred to as the pump pulse, is incident on a target monoatomic gas, it will ionize the atom or in otherwords drive a transition of a core electron to the continuum. As each harmonic can drive a single-photon continuum transition the possible final continuum states of the electron will be at the energies of the pump pulse harmonics minus the ionization potential of the atom as shown below. These electrons can be detected using an electron spectrometer. The intensity recorded on the spectrometer is directly proportional to the intensity of each harmonic in the probe pulse.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Pump Only.png\">\n",
    "\n",
    "***Figure:** The pink arrows represent single-photon transitions driven by the pump pulse at harmonic orders $q$ through $q+4$. The dotted lines represent continuum energy levels with the given energy given on the lefthand side where $\\omega$ is the infrared driving frequency, $q$ is the harmonic order, and $I_p$ is the target atom ionization potential. The grey slashed area at lower energies represent bound states of the target atom.*\n",
    "\n",
    "Now a second pulse, a probe pulse, at the same frequency ($\\omega$) as the driving pulse for HHG which produced the pump pulse is incident on the target gas at the same time as the pump pulse. This probe pulse is typically produced by splitting a portion of the HHG driving pulse off before HHG in order to have temporal coherence between teh pump and probe pulses. This probe pulse is not high enough frequency (energy) to drive core level transitions; however, it is able to drive single-photon transitions in the electron wavepacket produced by the pump pulse between continuum states. As the pump pulse produced electrons at harmonics of its fundamental frequency, and the probe pulse is at this same fundamental frequency, the continuum electrons will be driven between already occupied states as shown below.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Pump and Probe.png\">\n",
    "\n",
    "***Figure:** The pink arrows again represent the pump pulse at harmonic orders $q$ through $q+4$. The red arrows represent the probe pulse with frequency $\\omega$. \n",
    "\n",
    "With both pulses present, the electrons at harmonic $q$ will then be the product of a quantum interference between electrons driven directly to harmonic $q$ by the pump pulse, electrons driven directly to harmonic $q+1$ by the pump pulse and then **down** to harmonic $q$ by the probe pulse, and electrons driven directly to harmonic $q-1$ by the pump pulse and then **up** to harmonic $q$ by the probe pulse. The phase of each the three pathways is directly proportional to the phase of harmonics $q$, $q+1$, and $q-1$ of the pump pulse and the phase of the probe pulse. More specifically, the electron wavepacket driven directly to harmonic $q$ will have a phase directly proportional to the phase of harmonic $q$ of the pump pulse. The wavepacket driven directly to harmonic $q+1$ and then down to harmonic $q$ will have a phase proportional to harmonic $q+1$ of the pump pulse and the phase of the probe pulse. The wavepacket driven directly to harmonic $q-1$ and then up to harmonic $q$ will have a phase proportional to harmonic $q-1$ of the pump pulse and the phase of the probe pulse. By varying the time delay between the probe pulse and the pump pulse we can then vary the phase of the probe pulse (taking the phase of the pump pulse harmonics to be constant). This phase variation will therefore vary the quantum interference at each continuum state producing a sinusoidal *beating* as a function of time at each state at frequency $\\omega$ (the pump pulse fundamental frequency / probe pulse frequency). These electrons can again be detected using an electron spectrometer giving a direct observation of the *beating* or interference as shown below.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Spectrogram.png\">\n",
    "\n",
    "***Figure:** Electron spectrogram taken in the DiMauro lab using Argon as a target atom. The y-axis is energy (or frequency), with energy increasing upwards. The x-axis is time. The z-axis is intensity. Note the sinusoidal beating of the intensity at every even harmonic. The lower energy odd-harmonics are not beating in this spectrogram as the lower energy odd-harmonics of the pump pulse were adjusted to be of much higher intensity than the even-harmonics for experimental purposes. Therefore, the lower energy odd-harmonics are experiencing a beating but on top of a much stronger DC background that obscures the beating.\n",
    "\n",
    "The phase of the $\\omega$ beating at each continuum energy level can be retrieved by taking the Fourier transform at each energy level along the time axis. Using these phases and the intensity at each harmonic, PROOF is able to fully retrieve the phases of each harmonic in the pump pulse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "It can be shown that at a given energy level $\\hbar\\nu=\\hbar q\\omega - I_p$ corresponding to a direct transition form harmonic $q$, the intensity at a given energy level of the spectrogram is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "I(\\nu,\\tau)=I_0+I_{\\omega}+I_{2\\omega}+...\n",
    "\\end{equation}\n",
    "\n",
    "$I_0$ is given by a direct transition to the energy level and is therefore independent of $\\omega$ as it does not involve any transitions driven by the probe pulse. Hence, $I_0$ does not oscilalte and is a DC background. $I_{\\omega}$ is given by a transition from an adjacent higher or lower energy level and hence involves a single-photon transition driven by the probe pulse and wil beat at $\\omega$. $I_{2\\omega}$ is given by a two-photon transition driven by the probe pulse where an electron from the energy level $q+2$ can be driven to energy level $q$ and similarily upwards from $q-2$. Hence, this term will beat at $2\\omega$. In PROOF measurements, we tune the probe pulse intensity to be much lower than the probe pulse intensity $~10^{11} \\frac{W}{cm^2}$, significantly lowering the probability of a two-photon (and higher) transitions from higher or lower energy levels and allowing us to ignore the $I_{2\\omega}$ and higher terms. This assumption was verified, although the results are not included in this notebook, by comparing the Fourier amplitude at $\\omega$, $2\\omega$, $3\\omega$, etc. when Fourier transforming at each energy level along the delay axis of the electron spectrogram. \n",
    "\n",
    "The one-photon intensity can then be expanded as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{\\omega}(\\nu) = 2\\phi_0 \\sqrt{(I_\\nu)} \\left(\\sqrt{(I_+)} \\sin\\left(\\omega \\tau + \\phi_\\nu - \\phi_+\\right) - \\sqrt{(I_-)}\\sin\\left(\\omega\\tau - \\phi-\\nu + \\phi_-\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $I_\\nu$ is the intensity at the energy levels $\\hbar\\nu$, $I_\\pm$ is the intensity at the adjacent energy levels $\\hbar(\\nu\\pm\\omega)$ where the one-photon transitions are being driven from, $\\phi_\\nu$ is the phase of the pump pulse harmonic corresponding to a direct transition to energy level $\\hbar\\nu$, $\\phi_\\pm$ is the phase of the pump pulse harmonic corresponding to a direct transition to energy level $\\hbar(\\nu\\pm\\omega)$, and $\\phi_0 = \\frac{\\nu E_0(t)}{\\omega^2}$ with $E_0(t)$ the time-dependent electric field amplitude of the HHG driving pulse.\n",
    "\n",
    "Using trigonometric identities this intensity can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "I_{\\omega}(\\nu) = 2\\phi_0 I_\\nu\\gamma\\sin(\\omega\\tau+\\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\gamma$ is the modulation depth and $\\alpha$ is the phase angle, given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tan(\\alpha) = \\frac{\\sqrt{(I_+)}\\sin(\\phi_{\\nu}-\\phi_+)+\\sqrt{(I_-)}\\sin(\\phi_{\\nu}-\\phi_-)}{\\sqrt{(I_+)}\\cos(\\phi_{\\nu}-\\phi_+)-\\sqrt{(I_-)}\\cos(\\phi_{\\nu}-\\phi_-)}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\gamma^2 = \\frac{1}{I_\\nu}\\left(I_++I_--2\\sqrt{I_+I_-}\\cos(\\phi_--\\phi_+)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Upon inspection of *Eq. 4* and *Eq. 5*, it is shown that the modulation depth and phase angle are directly retrieved from the Fourier transform along the delay axis at a given energy of the electron spectrogram, the phase angle being the phase of the modulation. Furthermore, these are functions of the pump pulse harmonic's phases, $\\phi_\\nu$ and $\\phi_\\pm$, the values we are hoping to retrieve with PROOF. Thus, using a genetic algorithm with inputs of the phase angles for a set of energy levels and the corresponding intensities at these energy levels, PROOF attempts to retrieve the value of $\\phi_\\nu$ for each harmonic.\n",
    "\n",
    "This derivation from the dissertation: Khan, Sabih Ud Din. “Generation of Short and Intense Attosecond Pulses.” Kansas State University, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Genetic Algorithms\n",
    "\n",
    "A genetic algorithm is a stochastic method based off of biological natural selection to solve optimization problems. By starting with an initial population, where each member of the population represents a solution to the optimization problem, the algorithm builds subsequent generations using the previous generations as parents. The parents selected to build each new member of a population are based of a fitness test of each member of the parent population with the most fit being more likely to reproduce and pass their genes to the subsequent generation. Additionally, each member of a new generation experiences a chance of being randomly mutated. With each generation, the algorith *evolves* towards a solution until a convergence condition has been met. Genetic algorithms typically consist of four steps with the first step performed once and 2-4 being repeated until convergence:\n",
    "\n",
    "1. Initialization of the population\n",
    "2. Fitness test\n",
    "3. Reproduction\n",
    "4. Mutation\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "PROOF is a genetic algorithm that aims to solve for $\\phi_\\nu$ of each pump pulse harmonic by optimizing the phase angle $\\alpha$ of each harmonic. Each member of a population is made up of a number of genes. The number of genes in each member is equal to the number of harmonics in the phase angle and intenisty data given to PROOF. Hence, each gene represents the phase for a given harmonic and each member is a different solution to the phase characterization problem. The fitness is determined by calculating a phase angle using the phases for a given member and comparing these to the experimental phase angles given to PROOF. Selection of parents are then based off of this fitness value. Mutation is performed randomly by having each phase in each population member having a given chance to be randomly augmented. Once the fitness value reaches a convergence treshold, the algorithm returns the phases for the population member with the greatest fitness at that generation. Each step of PROOF is outlined in detail below with corresponding code.\n",
    "\n",
    "## Algorithm Initialization\n",
    "\n",
    "Notation: **variables are written in bold font** and *classes are written in italic font*\n",
    "\n",
    "Additionally, each code block in this jupyter notebook corresponds to a seperate .py file used in the algorithm. The name of each .py file is written at the top of the codeblock in this notebook. Additonally, all codeblocks can be found in the Codes subfolder of the PROOF\n",
    "\n",
    "My PROOF code is written as a class named *PROOF* with corresponding function calls for each of the four steps above. \n",
    "\n",
    "The initialization of this class ( \\__init__ ) has inputs **population_size** which is the number of members of each population, **elite_factor** which is the percentage of the population that will be cloned to the subsequent generation (explained further under reproduction), and **mutation_rate** which is the percent chance of a population member to mutate (explained further under mutation).\n",
    "\n",
    "During initialization of the class *PROOF* the experimental data is imported as a subclass named *data*. This subclass's script named *extract_data* is defined in the script *classes.py*. This sublcass, *data* has three callables: **intensity**, square root of the intensity (**intensity_sqrt**), and **alpha**. All three of these are from the experimental data. The square root is also saved in order to save computation time of recalculating the square root for each call of the array.\n",
    "\n",
    "Also during the initialization of the class *PROOF* the number of harmonics is saved as a constant (**nHarm**) calculated from the extracted experimental data and the population size (**pop_size**) is as a constant taken from the *PROOF* class input.\n",
    "\n",
    "Finally three additional subclasses (*family_tree*, *roulette*, *constants*) are also created. The first two subclasses usage will be defined later. The *constants* subclass just constants for the algorithm which are the **elite_factor** and **mutation_factor** which were inputs of the *PROOF* class as well as the constants **seed3** and **seed4** whose usage will be explained later. All three of these subclasses' scripts are also defined in the script *classes.py*. \n",
    "\n",
    "Beyond the intialization of the *PROOF* class, the class also has four function calls coresponding to the four steps above. Each of these steps and corresponding code will be explained in its own subsection later.\n",
    "\n",
    "The class is displayed below as well as the *classes.py* script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classPROOF.py\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "PROOF class for running the algorithm. Call this to intialize the process\n",
    "and arrays. Then call the four functions to continue through the PROOF\n",
    "algorithm\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from Codes import initialization\n",
    "from Codes import fitness\n",
    "from Codes import classes\n",
    "from Codes import reproduction\n",
    "from Codes import mutation\n",
    "\n",
    "\n",
    "class PROOF:\n",
    "    def __init__(self,population_size,elite_factor,mutation_rate):\n",
    "        \n",
    "        \"\"\"\n",
    "        The inputs of this initializaiton are the population size, the\n",
    "        percentage of each generation to clone exempt from mutation\n",
    "        (elite_factor) and the percentage chance for any mutation to \n",
    "        occur (mutation_rate).\n",
    "        \n",
    "        Each subclasses callables explained in function where used\n",
    "        \"\"\"\n",
    "        \n",
    "        #extract data from subdirectory /inputDATA as data class\n",
    "        #must be labeled intensity.txt and alpha.txt\n",
    "        #callables: intensity, intensity_sqrt, and alpha\n",
    "        self.data = classes.extract_data()\n",
    "        \n",
    "        #initialize parameters needed as PROOF class\n",
    "        self.nHarm = len(self.data.intensity)+2     #number of harmonics     \n",
    "        self.pop_size = population_size             #population size of each generation\n",
    "        \n",
    "        #intialize arrays needed as family_tree class for each generation\n",
    "        #callables: parents, population, fitness, objective\n",
    "        self.family_tree = classes.family_tree(self.pop_size,self.nHarm)\n",
    "        \n",
    "        #initialize arrays needed for fitness calculation and reproduction as roulette class\n",
    "        #callables: alias, prob\n",
    "        self.roulette = classes.roulette(self.pop_size,self.nHarm,self.family_tree.fitness)\n",
    "        \n",
    "        #initialize constants needed for algorithm as constants class\n",
    "        #callables: elite_factor, mutation_rate, seed3, seed4\n",
    "        self.constants = classes.constants(elite_factor,mutation_rate)\n",
    "        \n",
    "    def initialize_population(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initially create the population using new polynomial of random order\n",
    "        for each member. Input of polynomial is array of integers evenly\n",
    "        spaced around 0 of length number of harmonics\n",
    "        \"\"\"\n",
    "        \n",
    "        x = np.arange(-self.nHarm/2,self.nHarm/2)   #intializaiton polynomial input\n",
    "        seed1 = np.random.RandomState()             #seed for random polynomial order selection\n",
    "        seed2 = np.random.RandomState()             #seed for random coefficient selection\n",
    "        \n",
    "        for chrom_num in range(self.pop_size):      #loops over each member in population, creating it\n",
    "            self.family_tree.population[chrom_num] = initialization.randpoly(x,seed1,seed2)     #initial population saved here\n",
    "\n",
    "                \n",
    "    def calculate_fitness(self):\n",
    "        \n",
    "        alpha = fitness.calc_alpha(self.family_tree.population,self.data.intensity_sqrt)            #calculates alpha for each gene of each population member\n",
    "        self.family_tree.fitness, self.family_tree.objective = fitness.calc_fitness(alpha,self.data.alpha,self.data.intensity)          #calcluates objective and fitness values for each population member\n",
    "        \n",
    "    def reproduce(self):\n",
    "                        \n",
    "        self.family_tree.parents = self.family_tree.population.copy()           #copy generation to parents so generation can be replaced\n",
    "                \n",
    "        self.roulette.prob, self.roulette.alias = reproduction.initialize_wheel(self.family_tree.fitness,self.pop_size)         #create alias and probability tables using Vose's Alias Method\n",
    "        \n",
    "        self.family_tree.population = self.family_tree.population[np.argsort(self.family_tree.fitness)]                         #sort population to allow for elite cloning\n",
    "        \n",
    "        for child in range(int(self.pop_size-round(self.pop_size*self.constants.elite_factor))):                                #loop over non-elite rows to produce new children\n",
    "            parent1 = reproduction.alias_selection(self.pop_size,self.roulette.prob,self.roulette.alias)\n",
    "            parent2 = reproduction.alias_selection(self.pop_size,self.roulette.prob,self.roulette.alias)\n",
    "            random_splice = random.randrange(1,self.nHarm)\n",
    "            self.family_tree.population[child,:random_splice] = self.family_tree.parents[parent1,:random_splice]\n",
    "            self.family_tree.population[child,random_splice:] = self.family_tree.parents[parent2,random_splice:]\n",
    "       \n",
    "    def mutate(self):\n",
    "        \n",
    "        self.family_tree.population = mutation.mutation (self.family_tree.population,self.constants.mutation_rate,self.pop_size,self.constants.elite_factor,self.nHarm,self.constants.seed3,self.constants.seed4)\n",
    "        self.family_tree.population = mutation.crossover (self.family_tree.population,self.constants.mutation_rate,self.pop_size,self.constants.elite_factor,self.nHarm)\n",
    "        self.family_tree.population = mutation.rotation (self.family_tree.population,self.constants.mutation_rate,self.pop_size,self.constants.elite_factor,self.nHarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes.py script which is shown below contains the initialization definitions for the various subclasses used.\n",
    "\n",
    "The *extract_data* class intakes the experimental data used in this algorithm. The data must be saved in the subfolder of inputDATA. The name of the intensity and alpha files are currently hardcoded into this script but in the future I plan to write these as inputs to the PROOF class.\n",
    "\n",
    "The other three classes defined here intialize arrays which will be used for later functions. *family_tree*'s variables are explained in initialization, *roulette*'s variables are explained in reproduction, and *constants* variables are explained in reproduction (**elite_factor**) and mutation (**mutation_rate*, **seed3**, **seed4**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classes.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "\n",
    "#imports data\n",
    "\n",
    "class extract_data:\n",
    "    def __init__(self):\n",
    "        \n",
    "        '''\n",
    "        class where experimental data is saved\n",
    "        '''\n",
    "            \n",
    "        iPath = getcwd() + \"/inputDATA/real_intensity.txt\"        #path where intensity data is saved\n",
    "        aPath = getcwd() + \"/inputDATA/real_phase.txt\"            #path where alpha data is saved\n",
    "        \n",
    "        #input intensity data by looping through lines of file\n",
    "        intensity_import = np.empty(0)\n",
    "        with open(iPath) as text:\n",
    "            for line in text:\n",
    "                intensity_import = np.append(intensity_import, float(line))\n",
    "        self.intensity_sqrt = np.sqrt(intensity_import)     #sqrt also saved in order to save computation time\n",
    "        self.intensity = intensity_import[1:-1]             #endpoints removed in order to save computation time on future calls in reproduction\n",
    "        \n",
    "        #input alpha data by looping through lines of file        \n",
    "        alpha_import = np.empty(0)\n",
    "        with open(aPath) as text:\n",
    "            for line in text:\n",
    "                alpha_import = np.append(alpha_import, float(line))\n",
    "        self.alpha = alpha_import[1:-1,None].transpose()    #endpoints and transpose removed in order to save computation time on future calls in fitness calculation\n",
    "        \n",
    "class family_tree:\n",
    "    def __init__(self,pop_size,nHarm):\n",
    "        self.parents = np.empty((pop_size,nHarm))           #parent population to current population being produced. using in reproduction\n",
    "        self.population = np.empty((pop_size,nHarm))        #current population used for that generation\n",
    "        self.fitness = np.empty(pop_size)                   #fitness for current population\n",
    "        self.objective = np.empty(pop_size)                 #objective for current population\n",
    "        \n",
    "class roulette:\n",
    "    def __init__(self,pop_size,nHarm,fitness):\n",
    "        self.alias = np.empty(pop_size)                     #alias array used for reproduction. see reproduction.py for more information\n",
    "        self.prob = np.empty(pop_size)                      #probability array used for reproduction. see reproduction.py for more information\n",
    "\n",
    "class constants:\n",
    "    def __init__(self,elite_factor,mutation_rate):\n",
    "        self.elite_factor = elite_factor                    #elite_factor used for cloning population\n",
    "        self.mutation_rate = mutation_rate                  #chance of population member being mutated\n",
    "        self.seed3 = np.random.RandomState()                #used for mutation. see mutation.py\n",
    "        self.seed4 = np.random.RandomState()                #used for mutation. see mutation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Initialization\n",
    "\n",
    "The first step of a genetic algorithm as well as the first defined function of the *PROOF* code is to initialize the population. This initialization is typically random, as done here. The number of members of the population is set by the variable **self.pop_size** which is defined as an input of the *PROOF* script. \n",
    "\n",
    "For all work done in this notebook the population size was set to 20000. As a genetic algorithm functions as guided random search algorithm, the larger the population size, the larger the search space and hence should lead to a more accurate answer in a shorter amount of runtime. The algorithm though performs many operations throughout on the population, so a larger population leads to a longer runtime. Therefore, I chose 20000 as it was a large enough population for accurate results but with a reasonable runtime. The runtime of the algorithm is further discussed later under optimization. \n",
    "\n",
    "Each member of the population will consist of a set of phases. The number of phases is set by the variable **self.nHarm** which is the number of harmonics in the experimental data input. Therefore, each phase of a member correspond to a phase of a different harmonic for the pump pulse that the experimental data is from. Therefore, each member of the population is a solution to the overall optimization problem. These phases will be called genes of the member from here forward.\n",
    "\n",
    "In order to randomly initialize the population, for each member a polynomial of random order between 1 and 10 is produced with coefficients equal to a randomly chosen number between $-\\pi$ and $\\pi$ divided by the $10^{order}$ where order is the order corresponding to that coefficient. The subset $(-\\pi,\\pi)$ is used as phases are only meaning to modular order $2\\pi$. The dividing factor is to ensure the output of the random polynomial is within $(-\\pi,\\pi)$ for each gene. This polynomial therefore can be written as:\n",
    "\\begin{equation}\n",
    "P(x) = \\sum_{i=0}^{n} \\frac{Rnd(-\\pi,\\pi)}{10^i} x\n",
    "\\end{equation}\n",
    "where $n$ is the random order.\n",
    "\n",
    "The input of this polynomial is an array of evenly spaced values between **-self.nHarm/2** and **self.nHarm/2** of length **self.nHarm** defined as **x**. Thus, $P(x)$ acts element-wise on **x**.\n",
    "\n",
    "Initialization occurs by calling the *PROOF* class function initialize_function(self) defined above in classPROOF.py. This function creates **x** and stores two random seeds **seed1** and **seed2** for random number generators. **seed1** is used for the random number generator to produce the random polynomial order and **seed2** is used for the random value selection in $(-\\pi,\\pi)$. The function then loops through each member of the population (i.e. 20000 times for this notebook), intializing each member using the function randpoly defined in intialization.py. As initializaiton only occurs once during the algorithm, no effort to remove this for loop was made.\n",
    "\n",
    "The population members are stored in the array **self.family_tree.population** which is initialized when the *family_tree* subclass is intialized during the *PROOF* initialization.\n",
    "\n",
    "initialize_function(self) can be found above in the classPROOF.py script. Below is the initialization.py script which contains one function randpoly(x,seed1,seed2). This function intakes the **x** array, **seed1**, and **seed2**. Using these seeds it randomly selects a polynomial order between 1 and 10 and intializes a coefficients array of length equal to the order. Then it randomly creates each coefficient using **seed2**. Then using numpy, it turns this coefficient array into a function which finally acts element-wise along x, returning these values from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initialization.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from math import pi\n",
    "    \n",
    "def randpoly(x,seed1,seed2):\n",
    "    n = seed1.randint(1,10)               #randomly select polynomial order between 1 and 10\n",
    "    coefficients = np.empty(n)            #initialize coefficient array\n",
    "    for order in range(n):\n",
    "        coefficients[n-order-1] = seed2.uniform(-pi,pi) / (10**order)           #randomly produce coefficients\n",
    "    poly = np.poly1d(coefficients)       #create polynomial function from coefficients with numpy.array input\n",
    "    return poly(x)                       #act polynomial element-wise on x. x defined in classPROOF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Calculation\n",
    "\n",
    "After initialization the fitness is calculated for the entire population. For each member of the population, *Eq. 4* from the derivation section is used to calculate an $\\alpha$ value for each gene of the member excluding the endpoint genes as *Eq. 4* requires both the given gene and the directly adjacent genes. These $\\alpha$ values are compared against the experimentally measured $\\alpha$ values saved as **self.data.alpha** to calculate fitness.\n",
    "\n",
    "Specifically, fitness for each member is calculated as the least squares error of that member's $\\alpha$ values against the experimental $\\alpha$ values weighted by the experimental spectral intensities given as:\n",
    "\n",
    "\\begin{equation}\n",
    "F(i) = \\frac{\\frac{1}{R(i)}}{Max\\left(\\frac{1}{R(i)}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where $i$ is the $i^{\\text{th}}$ population and $R(i)$ is the objective function which is the weighted least squares error of $\\alpha$ values given as:\n",
    "\n",
    "\\begin{equation}\n",
    "R(i)=\\sum_{\\nu}I(\\nu)\\left(\\alpha(\\nu)-\\alpha_{exp}(\\nu)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "where $I(\\nu)$ is the intensity at energy level (or gene) $\\nu$ and $\\alpha_{exp}$ is the experimental phase angle at level $\\nu$ and $\\alpha$ is the alpha value for each gene of the population member.\n",
    "\n",
    "The fitness value is calculated using the recipricol of the objective function since as members of a population converge towards a correct solution, the objective function will reduce; however, with convention of genetic algorithms, the fitness value should increase as solutions improve. Additionally, the fitness value is normalized for each generation so that each fitness value can be interpreted as a probability to reproduce which will be further explained during reproduction.\n",
    "\n",
    "Fitness calculation occurs by calling the *PROOF* class function calculate_fitness(self) defined above in classPROOF.py. First alpha values for each member's genes are calculated using the function calc_alpha(self.family_tree.population,self.data.intensity_sqrt) defined in fitness.py. This function outputs alpha values for the entire population. These alpha values are then fed into the function calc_fitness(alpha,self.data.alpha,self.data.intensity) defined in fitness.py. This function outputs the fitness and objective values for each population member which are stored in **self.family_tree.fitness** and **self.family_tree.objective** which were intialized when the *family_tree* subclass was initialized during intializaiton of the *PROOF* class. Both the fitness and objective value are stored for algorithm optimization as well as for convergence conditions explained later.\n",
    "\n",
    "calculate_fitness(self) can be found above in the classPROOF.py script. Below is the fitness.py script which contains the two functions calc_alpha(self.family_tree.population,self.data.intensity_sqrt) and calc_fitness(alpha,self.data.alpha,self.data.intensity)\n",
    "\n",
    "calc_alpha intakes the population as well as the square root of the intensity. Using these values it calculates $y$ which is the numerator of *Eq. 4* and $x$ which is the denominator of *Eq. 4*. It finally returns the $\\arctan$ of $\\frac{y}{x}$ as defined in *Eq. 4*.\n",
    "\n",
    "calc_fitness then intakes the alpha values, the experimental alpha values, and the intensity. It first calculates the least squares error for each member (*Eq. 2* of thi section. Then it takes the reciprocal before implementing the fitness formula (*Eq. 1* of this section). The objective is done as seperate step, so it can be output as a separate variable.\n",
    "\n",
    "Both of these calculations are done as array manipulations on slices of the population array in order to reduce runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fitness.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#see jupyter_notebook for alpha and fitness formula used\n",
    "\n",
    "def calc_alpha (population,intensity_sqrt):\n",
    "    y = intensity_sqrt[2:]*np.sin(population[:,1:-1]-population[:,2:]) + intensity_sqrt[:-2]*np.sin(population[:,1:-1]-population[:,:-2])\n",
    "    x = intensity_sqrt[2:]*np.cos(population[:,1:-1]-population[:,2:]) - intensity_sqrt[:-2]*np.cos(population[:,1:-1]-population[:,:-2])\n",
    "    return np.arctan2(y,x)\n",
    "\n",
    "def calc_fitness (alpha,alpha_exp,intensity):\n",
    "    objective = np.dot(np.square(alpha-alpha_exp),intensity)\n",
    "    reciprocal_objective = np.reciprocal(objective)\n",
    "    return reciprocal_objective*np.max(reciprocal_objective), objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction\n",
    "\n",
    "As stated before, genetic algorithms loop over a number of generations with each generation containing an \"entirely\" new population as it evolves towards a generation. This new population is only randomly created in the first generation though. Each subsequent generation population is produced using combinations of the previous population's members. Here forward the current generation population will be referred to as parents and the next generation population being created will be referred to as children.\n",
    "\n",
    "In order to create children of **self.pop_size** members for each generation that evolves towards a correct solutions, two methods are used. First, the top **self.constants.elite_factor** of the parents are cloned (or copied) into the children. The members chosen for cloning are based off of their fitness values. Therefore, the **self.pop_size**$\\times$**self.constants.elite_factor** members with the highest fitness values will be automatically cloned into the children. In order to fill in the rest of the children, reproduction is performed. In reproduction, to create each new child, two members of the parent population (parent 1 and parent 2) are selected. Then a random slice point within $(0,\\textbf{self.nHarm})$ is chosen. The genes for indices :(slice point) of parent 1 are copied as genes :(slice point) of the child. The genes for indices (slice point): are copied as genes (slice point): of the child. Now, a child with **self.nHarm** is created. This process is repeated for each child until the entirety of children is created.\n",
    "\n",
    "Parent selection is done using what is termed *roulette wheel selection*. This selection method is a weighted random choice from the parent population based off fitness values of the parent population. Thus, the parents with the highest fitness values have the highest chance of being chosen as a parent for a child. The only condition is parents cannot reproduce with themselves. By using this weighted choice, the algorithm mimics natural selection, where the strongest genes continue into the children until the algorithm evolves to a solution.\n",
    "\n",
    "### Vose's Alias Method\n",
    "\n",
    "Roulette wheel selection is implemented by using Vose's Alias Method which is the most efficient implementation method with O(n) initialization followed by O(1) parent selection. First I will explain the Alias Method followed by Vose's implementation. Imagine you have a list of elements with associated probabilities and you would like to make a random choice of the elements based on the probablities as weights. Would you could do is create a histogram of each probability as shown below and then throw a dart at the histogram. If it hit a rectangle, that is your choice. If it hits white space then you throw again. As each rectangle is proportional in size to the probability this implements your weighted choice. \n",
    "\n",
    "<img src=\"files/Jupyter Figures/Histogram.png\">\n",
    "\n",
    "The issue is white space is a large portion of the graph, so you spend a large portion of time rethrowing. In order to guarantee a hit on every throw, first scale the rectangles up by multiplying by the number of elements and draw a threshold rectangle of height 1 as shown below\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Scaled Histogram.png\">\n",
    "\n",
    "Next we will cut some of the first rectangle to fill the fourth rectangel to threshold as shown below.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Histogram 3.png\">\n",
    "\n",
    "We will continue this process until the threshold rectangle is full as shown below.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Full Histogram.png\">\n",
    "\n",
    "Note how only two elements are in each column. Therefore, we can roll a dice between 0 and 4 to select a column and then another dice between 0 and 1. If the second dice value is less than or equal to the bottom rectangle value, we select that element, if it is not, then we select the upper rectangle. The upper rectangles are what are known as aliases, and the lower rectangles as probabilities. \n",
    "\n",
    "As is this algorithm has O(1) choice but initializaiton will be slow. In order to speed of initialization we begin with two worklists. One with the probability of the elements with scaled values greater than or equal to 1 and one with the probability of those less than 1. We take the probability of the element at the top small worklist and make it element one of the probability array. We then take the index of the top element of the large worklist and make it element one of the alias array. We subtract (1-probability(small)) element from the probability of the large element we use. If probability of large is still greater than one we leave it on top of the large pile. If is now less then one we move it to the small pile. We continue this iteration until the arrays are full as shown below:\n",
    "\n",
    "<img src=\"files/Jupyter Figures/vose 1.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/vose 2.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/vose 3.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/vose 4.png\">\n",
    "\n",
    "### Reproduction Implementation\n",
    "\n",
    "Reproduction in the PROOF algorithm occurs by calling the *PROOF* class function reproduce(self) defined above in classPROOF.py. First the **self.family_tree.parents** array is loaded by copying the **self.family_tree.population** as **self.family_tree.population** will become the children.\n",
    "\n",
    "Next, the probability (**self.family_tree.population**) and alias (**self.roulette.alias**) arrays are created using the initialize_wheel(self.family_tree.fitness,self.pop_size) function located in reproduction.py. Then **self.family_tree.population** is sorted based upon **self.family_tree.fitness** with the most fitness elements at the bottom. Finally, the children are created.\n",
    "\n",
    "To create the children, **self.family_tree.fitness** loops through the rows starting at row 0 through row (**self.pop_size**-round(**self.pop_size**$\\times$**self.constants.elite_factor**) replacing each row with a new child. As it does not loop through all rows, the remaining untouched rows are the elite members that were cloned as the array had been sorted with the most fit at the bottom.\n",
    "\n",
    "The two parents for each child are created using the alias_selection(self.pop_size,self.roulette.prob,self.roulette.alias) function in reproduction.py. Then a **random_splice** is chosen. Finally using this information, a new child can be created. At completion, **self.family_tree.population** is now the new generation.\n",
    "\n",
    "reproduce(self) can be found above in the classPROOF.py script. Below is the reproduction.py script which contains the two functions initialize_wheel(self.family_tree.fitness,self.pop_size) and alias_selection(self.pop_size,self.roulette.prob,self.roulette.alias)\n",
    "\n",
    "My implementation of Vose's Alias Method in initialize_wheel(self.family_tree.fitness,self.pop_size) does not use worklists as appending and deleting elements can be computationally heavy. Rather, I keep track of the large, small, and next small indices. Large and next small will never go down in value but small can move backwards if a large element is moved to the small worklist. First, alias and probability arrays are initialized and probabilities are created from fitness values. Next, the initial small and large element indices are identified. The algorithm then walks through the fitness produced probability list adding appropriate elements to the alias and probability array. Important to note is the population values's indices in **self.roulette.prob** are the same indices as the parents that supplied those probability values. The alias and probability arrays are output and stored in **self.roulette.prob** and **self.roulette.alias**\n",
    "\n",
    "alias_selection(self.pop_size,self.roulette.prob,self.roulette.alias) selects a parent based on the produced alias and probability tables. It first randomly selects a column (or index of the probablity.alias tables). Then it randomly chooses a number between 0 and 1. If the number is less than the value corresponding to the selected index of **self.roulette.prob**, the current index is returned which corresponds to the parent that supplied that probability. Otherwise the element in **self.roulette.alias** corresponding to the index is returned. That element is the parent index for that alias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reproduction.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def initialize_wheel (fitness,pop_size):\n",
    "    \n",
    "    alias = np.empty(pop_size)                              #intialize alias\n",
    "    prob = np.empty(pop_size)                               #initialize probability\n",
    "    probability = fitness/np.sum(fitness) * pop_size        #produce probabilities from fitness\n",
    "    large = 0                                               #current large element\n",
    "    small = 0                                               #current small element\n",
    "            \n",
    "    while (large < pop_size and probability[large] < 1):    #find first large element\n",
    "        large += 1\n",
    "    while (small < pop_size and probability[small] >= 1):   #find first small element\n",
    "        small += 1\n",
    "    next_small = small + 1\n",
    "    \n",
    "    while (large < pop_size and small < pop_size):\n",
    "        prob[small] = probability[small]                    #place first small element in probability array\n",
    "        alias[small] = large                                #place first large alias in alias array\n",
    "        probability[large] = (probability[large] + probability[small]) - 1     #adjust alias probability\n",
    "        if (probability[large] >= 1 or next_small <= large):            #if current large element is still greater than one or next small element is small\n",
    "            small = next_small                              #advance small marker\n",
    "            while (small < pop_size and probability[small] >= 1):       #continue advancing small marker until finds a small element\n",
    "                small += 1\n",
    "            next_small = small + 1\n",
    "        else:\n",
    "            small = large                                   #small marker has reached large\n",
    "        while (large < pop_size and probability[large] < 1):    #advance large marker to next large\n",
    "            large += 1\n",
    "\n",
    "    \n",
    "    while (large < pop_size):                               #in case large elements left equal one and small is exhasuted, add all to alias\n",
    "        if (probability[large] < 1):\n",
    "            large += 1\n",
    "            continue\n",
    "        prob[large] = 1\n",
    "        alias[large] = large\n",
    "        large += 1   \n",
    "    \n",
    "    if (small < pop_size):                                  #in case small elements left equal one\n",
    "        prob[small] = 1\n",
    "        alias[small] = small\n",
    "        small = next_small\n",
    "        while (small < pop_size):\n",
    "            if (probability[small] > 1):\n",
    "                small += 1\n",
    "                continue\n",
    "            prob[small] = 1\n",
    "            alias[small] = small\n",
    "            small += 1\n",
    "    \n",
    "    return prob, alias.astype(int)\n",
    "\n",
    "def alias_selection(pop_size,prob,alias):\n",
    "    side = random.randrange(0,pop_size)                     #choose column randomly\n",
    "    if random.random() < prob[side]:                        #choose value between 0 and 1 and select alias accordingly\n",
    "        return side\n",
    "    else:\n",
    "        return alias[side]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation\n",
    "\n",
    "The final element of the genetic algorithm is mutation of the new non-elite population. Mutation is needed in order to stop convergence on local minima as well as to constantly add new solutions to the population pool. Six different types of mutation are utilized in PROOF as it yields the fastest and most accurate convergence as shown later. The mutations used are:\n",
    "\n",
    "1. Single-point mutation\n",
    "2. Multi-point mutation\n",
    "3. Uniform mutation\n",
    "4. Single-point crossover\n",
    "5. Multi-point crossover\n",
    "6. Rotation\n",
    "\n",
    "In single-point mutation, each gene in each member of the population has a **self.constants.mutation_rate** chance of randomly mutating where a random number in $(-\\pi,\\pi)$ is addded to the gene. In multi-point mutation, each member of the population has a **self.constants.mutation_rate** chance of randomly having a continous portion of the member (i.e. gene 7-9) mutate where a random number in $(-\\pi,\\pi)$ is addded to the genes. The same number is added to each gene. In uniform mutation, each member of the population has a **self.constants.mutation_rate** chance of all their genes randomly mutating where a random polynomial produced in the same way as initialization is added. In uniform mutation, the member is essentially wiped out and replaced by a random new solution.\n",
    "\n",
    "In single-point crossover, two members of the population have a **self.constants.mutation_rate** chance of having two of their genes, one from each, randomly swap. In multi-point cross, two members of the population have a **self.constants.mutation_rate** chance of having the multiple genes swap. \n",
    "\n",
    "In rotation, a member of the population has a **self.constants.mutation_rate** chance of having their genes rotated along their axis by a randomly chosen amount between $(0,\\textbf{self.nHarm})$. \n",
    "\n",
    "Mutation in the PROOF algorithm occurs by calling the *PROOF* class function mutate(self) defined above in classPROOF.py. First single-point, multi-point, and uniform mutation occurs by calling the function mutation located in mutation.py. Next, both cross-over occur by calling the function crossover in mutation.py. Finally, rotation occurs by calling rotation in mutation.py.\n",
    "\n",
    "mutate(self) can be found above in the classPROOF.py script. Below is the mutation.py script which contains the mutation, crossover, and rotation function.\n",
    "\n",
    "All mutations function operate in the same loop which traverses all non-elite members. For each member a random number is drawn for each type of mutation. If that number is less than **self.constants.mutation** then the mutation occurs. For uniform mutation, randpoly(x,seed3,seed4) is used again from initialization.py. Note this intakes another two randomly generated seeds which are stored in *constants*. Cross-over continues the same-way but loops over only half the non-elite members as each member of the population has had a pair assigned to it. Finally, rotation occurs similarily.\n",
    "\n",
    "Due to the multi-point selections which have varying slice lengths, I was unable to optimize mutation much by rewriting it using arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mutation.py\n",
    "'''\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from Codes import initialization\n",
    "from math import pi\n",
    "\n",
    "def mutation (population,mutation_rate,pop_size,elite_factor,nHarm,seed3,seed4):\n",
    "    \n",
    "    x = np.arange(-nHarm/2,nHarm/2)         #create x array for uniform mutation\n",
    "    \n",
    "    for child in range(int(pop_size-round(pop_size*elite_factor))):     #loop through non-elite members\n",
    "        if random.random() < mutation_rate:                             #choose number between 0,1. if less than mutation rate then mutate\n",
    "            gene = random.randrange(0,nHarm)                            #select gen to mutate\n",
    "            population[child,gene] += random.uniform(-pi,pi)            #add random value\n",
    "            \n",
    "        if random.random() < mutation_rate:                             #decide mutation           \n",
    "            splice1 = random.randrange(0,nHarm)                         #decide splice point 1\n",
    "            splice2 = random.randrange(0,nHarm)                         #decide splice point 2\n",
    "            splice_beginning = min(splice1,splice2)                     #order splice points\n",
    "            splice_end = max(splice1,splice2)                           #order splice points\n",
    "            population[child,splice_beginning:splice_end] += random.uniform(-pi,pi)     #add random value\n",
    "            \n",
    "        if random.random() < mutation_rate:                             #decide mutation\n",
    "            population[child] += initialization.randpoly(x,seed3,seed4) #add random polynomial\n",
    "            \n",
    "    return population\n",
    "    \n",
    "def crossover (population,mutation_rate,pop_size,elite_factor,nHarm):\n",
    "    \n",
    "    cross_over_pairs = random.sample(range(int(pop_size-round(pop_size*elite_factor))),int((pop_size-round(pop_size*elite_factor))))        #selet crossover pairs randomly from non-elite members  \n",
    "\n",
    "    for child in range(int((pop_size-round(pop_size*elite_factor))/2)):                 #loop over non-elite pairs\n",
    "        if random.random() < mutation_rate:                                             #decide crossover\n",
    "            gene = random.randrange(0,nHarm)                                            #select cross-over gene\n",
    "            child_gene1 = population[cross_over_pairs[child],gene]                      #pull cross-over value 1\n",
    "            child_gene2 = population[cross_over_pairs[-1*(child+1)],gene]               #pull cross-over value 2\n",
    "            population[cross_over_pairs[child],gene] = child_gene2                      #swap value 1\n",
    "            population[cross_over_pairs[-1*(child+1)],gene] = child_gene1               #swap value 2\n",
    "            \n",
    "        if random.random() < mutation_rate:                                             #decide mutation\n",
    "            flip_genes = random.sample(range(nHarm),random.randrange(0,nHarm))          #decide genes to flip\n",
    "            child_genes1 = population[cross_over_pairs[child],tuple(flip_genes)]        #pull values 1\n",
    "            child_genes2 = population[cross_over_pairs[-1*(child+1)],tuple(flip_genes)] #pull values 2\n",
    "            population[cross_over_pairs[child],tuple(flip_genes)] = child_genes2        #swap values 1\n",
    "            population[cross_over_pairs[-1*(child+1)],tuple(flip_genes)] = child_genes1 #swap values 2\n",
    "            \n",
    "    return population\n",
    "\n",
    "def rotation (population,mutation_rate,pop_size,elite_factor,nHarm):\n",
    "    \n",
    "    for child in range(int(pop_size-round(pop_size*elite_factor))):                     #loop over non-elite population\n",
    "        if random.random() < mutation_rate:                                             #deicde mutation\n",
    "            rotation_length = random.randrange(0,nHarm)                                 #decide rotation_length\n",
    "            population[child] = np.roll(population[child],rotation_length)              #rotate\n",
    "            \n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Implementation\n",
    "\n",
    "In order to implement the above *PROOF* class, first it must be called and hence intialized. Then the population is called. Then fitness_calculation, reproduce, and mutate are looped over until convergence. For this algorithm convergence has been defined as at least 20000 generations must have passed. Then the change in the objective function must be less than $1e-7$ over the previous 1000 generations and hence an indication that the algorithm has stopped improving. Finally, it must meet these conditions for 10 consecutive sets of 1000 generations in order to ensure the algorithm is not stuck in a local minima. \n",
    "\n",
    "The *PROOF* class is implemented in proof_optimization.py and shown below.\n",
    "\n",
    "The **print_factor** specifies when to both save data as well as check convergence conditions. Conditions are not checked on every generation in order to improve computation time. As shown the implementation of the code is very simple. It involves initialization of the *PROOF* class followed by four function calls. The data saved at every **print_factor** generations is the current generation, the maximum fitness and corresponding the minimum objective, the current computation time, and the current best solution (i.e. phases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation size:  100\n",
      "Population size:  200\n",
      "Elite factor:  0.15\n",
      "Mutation factor:  0.15\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/danieltuthill/Desktop/School/OSU/6820/bigdata_6820-dtuthill/finalProject\\\\inputDATA\\\\intensity.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ad1d391c7d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mutation factor: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmutation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mproof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROOF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melite_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmutation_factor\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m#initialize PROOF class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msave_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprint_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnHarm\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m#initialize output data array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-1ae443589d0a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, population_size, elite_factor, mutation_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#must be labeled intensity.txt and alpha.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#callables: intensity, intensity_sqrt, and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#initialize parameters needed as PROOF class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/OSU/6820/bigdata_6820-dtuthill/finalProject/Codes/classes.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mintensity_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mintensity_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintensity_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/danieltuthill/Desktop/School/OSU/6820/bigdata_6820-dtuthill/finalProject\\\\inputDATA\\\\intensity.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "proof_optimization.py\n",
    "'''\n",
    "\n",
    "from Codes import classPROOF\n",
    "import time\n",
    "from os import getcwd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print_factor = 100                          #how often to print resutls\n",
    "\n",
    "generation_size = 100000\n",
    "population_size = 20000\n",
    "elite_factor = 0.15\n",
    "mutation_factor = 0.15\n",
    "\n",
    "#constants hard coded into jupyter notebook but taken as command line inputs for .py file\n",
    "#generation_size = int(sys.argv[1])\n",
    "#population_size = int(sys.argv[2])\n",
    "#elite_factor = float(sys.argv[3])\n",
    "#mutation_factor = float(sys.argv[4])\n",
    "\n",
    "print(\"Generation size: \",generation_size)\n",
    "print(\"Population size: \",population_size)\n",
    "print(\"Elite factor: \",elite_factor)\n",
    "print(\"Mutation factor: \",mutation_factor)\n",
    "\n",
    "proof = classPROOF.PROOF(population_size,elite_factor,mutation_factor)          #initialize PROOF class\n",
    "\n",
    "save_data = np.empty((int(generation_size/print_factor)+1,proof.nHarm+4))       #initialize output data array\n",
    "\n",
    "time_init = time.time()                                                         #time tracker\n",
    "\n",
    "proof.initialize_population()                                                   #initialize population\n",
    "\n",
    "i = 0                                                                           #saved data array index\n",
    "patience = 0                                                                    #convergence patience\n",
    "\n",
    "for generation in range(generation_size+1):                                     #loop oer generations\n",
    "    if generation % print_factor == 0:                                          #loop over generations and check convergence conditions\n",
    "        print(generation)\n",
    "        print(\"Time: \",time.time()-time_init)\n",
    "        print(\"Max Fitness: \",np.amax(proof.family_tree.fitness))\n",
    "        print(\"Max Objective: \",np.amin(proof.family_tree.objective))\n",
    "        save_data[i,0] = generation\n",
    "        save_data[i,1] = np.amax(proof.family_tree.fitness)\n",
    "        save_data[i,2] = np.amin(proof.family_tree.objective)\n",
    "        save_data[i,3] = time.time()-time_init\n",
    "        save_data[i,4:] = proof.family_tree.population[-1]\n",
    "        if (np.abs(save_data[i,2] - save_data[i-10,2]) < 0.0000001 and generation > 20000):     #check convergence conditions\n",
    "            patience += 1                                                       #add patience\n",
    "        else:\n",
    "            patience = 0\n",
    "        if patience == 10:                                                      #if met 10 times, end PROOF\n",
    "            break\n",
    "        i += 1\n",
    "    proof.calculate_fitness()\n",
    "    proof.reproduce()\n",
    "    proof.mutate()\n",
    "\n",
    "    \n",
    "#phase_data = open(getcwd() + \"/outputDATA/real.csv\",'ab')                       #save data\n",
    "    \n",
    "#np.savetxt(phase_data,save_data,delimiter=\",\",header=\"generation     fitness     objective     time     phase\")\n",
    "\n",
    "#phase_data.close()\n",
    "    \n",
    "#print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "In order to optimize the PROOF algorithm, I focused in on two parameters: elite factor and mutation rate. I tested 25 different combinations of these parameters by taking elite factor with values $(0.0,0.05,0.10,0.15,0.20)$ and mutation rate with the same values on a simulated data set In order to test the quality of each combination I used the following factors: convergence, final objective, and final phase error. Also important to note, I removed convergence conditions for all of these tests in order to stop any from early convergence.\n",
    "\n",
    "For each of the following convergence measurements, the algorithm was run five separate times for each parameter combination and the results were averaged. Each run consisted of a population of 20000 ran for 100000 generations. All calculations were also done using the Ohio Supercomputer.\n",
    "\n",
    "### Simulated Data Set\n",
    "\n",
    "The simulated data set I used is shown below where the orange curve is the inensity and the blue curve are the phases (**not** alpha values, i.e. the desired solution). \n",
    "\n",
    "<img src=\"files/Jupyter Figures/Data pi2 gradual actual.png\">\n",
    "\n",
    "I gave the intensities an envelope to more closely model the envelopes we see in the lab as seen in the electron spectrogram from the beginning of this notebook. Additionally, I wanted intensities on either end of the spectrum to go to zero in order to test the weighting in the least squares fit for the fitness calculation. I further made even harmonics 30% of odd harmonics as these are also our normal experimental conditions as again seen in the previous spectrogram.\n",
    "\n",
    "The phases I gave a quadratic chirp as this has been theoretically predicted for HHG. Additionally, the even harmonics having a slowly growing separation from the odds, culminating at a $\\frac{\\pi}{2}$ seperation is theoretically and predicted and experimentally observed [4]. \n",
    "\n",
    "To produce this simulated data I used file fakeDATA.py shown below.\n",
    "\n",
    "[4]: G. Laurent, W. Cao, I. Ben-Itzhak, and C. L. Cocke, \"Attosecond pulse characterization,\" Opt. Express 21, 16914-16927 (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fakeDATA.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi, sqrt, atan2, sin, cos, exp\n",
    "from os import getcwd\n",
    "\n",
    "#Initial parameters\n",
    "\n",
    "nHarm = 20 + 2              #Number of harmonics + 2 (2 extra end points needed for alpha calculation)\n",
    "\n",
    "#Create fake intensities\n",
    "\n",
    "intensity = np.zeros(nHarm)\n",
    "\n",
    "for i in range(0,nHarm):\n",
    "    #   intensity[i] = (i)**(2/3)            #intensity for random data\n",
    "    #   intensity[i] = 10*exp(-(i-nHarm/2.5)**2/20)+4*exp(-(i-nHarm/1.2)**2/10)          #intensity for continuous data\n",
    "        intensity[i] = 6*exp(-(i-nHarm/8)**2/3)+10*exp(-(i-nHarm/2.5)**2/15)+5*exp(-(i-nHarm/1.5)**2/10)     #intensity for harmonic data\n",
    "for i in range(1,nHarm,2):\n",
    "    intensity[i] *= 0.3           #add even/odd offset\n",
    "    \n",
    "\n",
    "\n",
    "#Create fake phases\n",
    "\n",
    "phases = np.zeros(nHarm)\n",
    "for i in range(0,nHarm):\n",
    "#    phases[i] = 0.02*nHarm/3+0.03*(i-nHarm/12)**2+0.0006*(i-nHarm/24)**3+0.00008*(i-nHarm)**4        #random phase\n",
    "#    phases[i] =  0.01*(i-nHarm/2)**2              #continous phase\n",
    "    phases[i] = 0.01*(i-nHarm/5)**2                #harmonic phase\n",
    "    if i % 2 == 1:\n",
    "        phases[i] += (i/nHarm)*pi/2                #add offset\n",
    "    \n",
    "#create alphas based on intensities and phases\n",
    "\n",
    "alphas = np.zeros(nHarm-2)\n",
    "\n",
    "for i in range(1,nHarm-1):\n",
    "    y = sqrt( intensity[i+1] ) * sin(phases[i] - phases[i+1] ) + sqrt( intensity[i-1] ) * sin(phases[i] - phases[i-1] )\n",
    "    x = sqrt( intensity[i+1] ) * cos(phases[i] - phases[i+1] ) - sqrt( intensity[i-1] ) * cos(phases[i] - phases[i-1] )\n",
    "    alphas[i-1] = atan2(y,x)\n",
    "    \n",
    "#write values to files\n",
    "    \n",
    "iPath = getcwd() + \"/inputDATA/intensity_pi2_actual_grad.txt\"\n",
    "pPath = getcwd() + \"/inputDATA/phase_pi2_actual_grad.txt\"\n",
    "aPath = getcwd() + \"/inputDATA/alpha_pi2_actual_grad.txt\"\n",
    "\n",
    "with (open(iPath, 'w')) as text:\n",
    "    for i in range(1, nHarm - 1):\n",
    "        text.write(str(\"%.6f\" %  intensity[i]))\n",
    "        if i < nHarm - 2:\n",
    "            text.write(\"\\n\")\n",
    "            \n",
    "with (open(pPath, 'w')) as text:\n",
    "    for i in range(1, nHarm - 1):\n",
    "        text.write(str(\"%.6f\" %  phases[i]))\n",
    "        if i < nHarm - 2:\n",
    "            text.write(\"\\n\")\n",
    "\n",
    "with (open(aPath, 'w')) as text:\n",
    "    for i in range(0, nHarm-2):\n",
    "        text.write(str(\"%.6f\" %  alphas[i]))\n",
    "        if i < nHarm - 2:\n",
    "            text.write(\"\\n\")\n",
    "            \n",
    "plt.plot(phases)\n",
    "plt.plot(intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Error\n",
    "\n",
    "This obvious first metric is the final error in the calculated phase vs. the expected phase for each parameter combination. This is shown below in two figures. The first figure is all the final phase calculations on top of each other. The second plot of the average phase error not including the endpoints as the endpoints are not directly factored into the objective calcluation (as alpha is 2 less in length than the phases).\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Phase.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Phase Error.png\">\n",
    "\n",
    "Overall all factors do well outside of those where the elite factor or mutation factor is zero. This is obviously expected as when the elite factor is zero, although reproduction factors in fitness values, the entire best solution for each generation is being thrown out every generation. For mutation factor being zero, this heavily constrains your search space to just those values produced during the first generation's intialization, so low performance is expected.\n",
    "\n",
    "## Final Objective\n",
    "\n",
    "Next, the metric I observed was the final objective value reached as this will communicate if the algorithm experienced early convergence. Two plots are shown below for this. The first plot shows the objective for each generation and the second shows the final objective. Even more evident in the second plot is again that elite factor or mutation factor lead to very poor results; however, most other combinations return satisfactory results\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Objective.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Final Objective.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Time\n",
    "\n",
    "Convergence is the third factor I investigated. Although convergence was not activated for these tests, I retroactively calculated when convergence was met for each set. Three plots are shown below for this. The first plot shows the total time for each parameter against generation. The second plot shows the convergence generation and the third plot the computation time when the algorithm would have converged. It is now clear that the search space for optimal parameters is reduced from a just nonzero factors to either elite factor = 0.15 and mutation factor = 0.1 or elite factor = 0.15 and mutation factor = 0.15. I chose to use both at 0.15 as it returned a slightly lower overall error.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Time.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Convergence.png\">\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Convergence Time.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation type\n",
    "\n",
    "After optimizing mutation and elite factors, I then tested types of mutation. I ran the algorithm again with a population of 20000 and a maximum generation of 100000, this time though with convergence, for the following combinations: mutation + crossover + rotation, only mutation, only crossover, only rotation. Results are an average of five runs for each combination. The results are as follows:\n",
    "\n",
    "|                | Phase Error | Final Objective | Convergence Generation |\n",
    "|----------------|-------------|-----------------|------------------------|\n",
    "| All            | 0.03326     | 7.1099e-06      | 42280.0                |\n",
    "| Mutation only  | 0.02385     | 3.9156e-06      | 51850.0                |\n",
    "| Crossover only | 0.06500     | 0.2044          | 30162.5                |\n",
    "| Rotation Only  | 0.05061     | 1.86490e-05     | 35500.0                |\n",
    "\n",
    "As shown, all combinations reach a low phase error and decent convergence generation, but crossover only is clearly stuck in a local minima. As all led to the lowest objectie, I chose to continue using all mutations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data\n",
    "\n",
    "I finally ran the optimized data set on four other simulated data sets and a real data set.\n",
    "\n",
    "### Simulated 1\n",
    "\n",
    "The first simulated data test I used is shown below. This is almost identical to the set optimized on except the phase offset between evens and odds is always at $\\frac{\\pi}{2}$. I chose this as papers outside of citation [4] have claimed an observation of this offset. The results for five averaged runs shown below the plot show a success.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Data pi2 gradual.png\">\n",
    "\n",
    "| Phase Error | Final Objective | Convergence Generation |\n",
    "|-------------|-----------------|------------------------|\n",
    "| 0.0204      | 1.7297e-06      | 25320.0                |\n",
    "\n",
    "### Simulated 2\n",
    "\n",
    "The second simulated data test I used is shown below. This is almost identical to the set optimized on except there is no phase offset. I chose this just out of curiosity although it is not physically realistic. The results for five averaged runs shown below the plot show a success.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Data pi2.png\">\n",
    "\n",
    "| Phase Error | Final Objective | Convergence Generation |\n",
    "|-------------|-----------------|------------------------|\n",
    "| 0.0019      | 6.3534e-05      | 22800.0                |\n",
    "\n",
    "### Simulated 3\n",
    "\n",
    "The third simulated data test I used is shown below. This is representative of a continous pulse (i.e. not harmonics but smooth in the spectral domain) as the even and odds have the same intensity and there no phase offsets. The results for five averaged runs shown below the plot show a success.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Data cont.png\">\n",
    "\n",
    "| Phase Error | Final Objective | Convergence Generation |\n",
    "|-------------|-----------------|------------------------|\n",
    "| 0.0017      | 2.8167e-05      | 42080.0                |\n",
    "\n",
    "### Simulated 4\n",
    "\n",
    "The fourth simulated data test I used is shown below. This is a non-physical result that was randomly generated. The results for five averaged runs shown below the plot show a success.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Data rand.png\">\n",
    "\n",
    "| Phase Error | Final Objective | Convergence Generation |\n",
    "|-------------|-----------------|------------------------|\n",
    "| 0.0674      | 1.9598e-06      | 24680.0                |\n",
    "\n",
    "### Real\n",
    "\n",
    "The fifth data test I used is real data taken in the lab using a 1525 nm driving laser and an Argon Target gas. HHG was also performed in Argon. As I do not know the correct solution, I can only use the final objective value which tends to infer that the algorithm was not successful. I also tried it using other driving wavelengths from the set experimental run and received similar results. I believe the failure to be not due to the code but rather inherent to the algorithm. The algorithm has a heavily reliance on the intensities; however, we believe our electron spectrometer to have a transmission function which we do not know. The phase though is not affected by this transmission function. Therefore, this disagreement could cause a failure of the algorithm as we have seen this same effect using other algorithms before. Despite this failure, the five runs are still shown below and the final statistics.\n",
    "\n",
    "<img src=\"files/Jupyter Figures/Real.png\">\n",
    "\n",
    "| Final Objective | Convergence Generation |\n",
    "|-----------------|------------------------|\n",
    "| 0.0856          | 23460.0                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Codes\n",
    "\n",
    "Additional files can be found in the PROOF folder. The analysis scripts contains the scripts used to produce the plots above and the optimization values. The bash scripts contains scripts used for supercomputer submission as well as the output work files. outputDATA is where the data is output to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
